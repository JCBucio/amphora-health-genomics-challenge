{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning challenges for genomics\n",
    "### 2022 Amphora Health's Data Challenge Internship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the 2022 Amphora Health’s Data Challenge Internship is to explore the genomic information that exists in our cells and develop machine learning models that can predict the risk of developing or acquiring a given disease.\n",
    "\n",
    "The goal of this challenge is to test the candidate’s ability to deliver a fully functional computational genomics product, which includes training a machine learning model, evaluating its accuracy, and testing it with new input from patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Your first task is to merge all the files into a single table to construct a merged genotype file for several individuals, this will become your training dataset.\n",
    "\n",
    "**Task 2.** Read from the ancestries file and extract the column called “Superpopulation Code”. Augment your merged file to include this new column. Each ancestry will be your target vector for the model. In other models, this target vector can be the presence or absence of disease (e.g. diabetes or not, cancer or not, etc.)\n",
    "\n",
    "**Task 3.** Split your database into 80% for training and 20% for testing. You will have to do a 10-fold cross-validation on the training set.\n",
    "\n",
    "**Task 4.** Train a machine learning model for a binary target (one per ancestry). For example, if a participant has African ancestry or not, another model for Asian ancestry, and a third model for European Ancestry.\n",
    "\n",
    "**Task 5.** Evaluate the accuracy of prediction using the area under the curve (AUC) for each model using 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code necessary to perform the run of all the jupyter notebooks in the project. To make this possible I am using the `papermill` package. Each notebook accomplish a different task from the ones defined above. They are separated as follows:\n",
    "\n",
    "- `src/main.ipynb`: This notebook contains the code to run all the notebooks in the project.\n",
    "- `src/0_preprocessing.ipynb`: This notebook contains the code to merge all the files into a single table to construct a merged genotype file for several individuals, this will be used for the training datasets.\n",
    "- `src/1_data-augmentation.ipynb`: This notebook contains the code to read from the ancestries file and extract the column called “Superpopulation Code”. Augment your merged file to include this new column. Each ancestry will be your target vector for the model. In other models, this target vector can be the presence or absence of disease (e.g. diabetes or not, cancer or not, etc.)\n",
    "- `src/2_split-and-training.ipynb`: This notebook contains the code to split the database into 80% for training and 20% for testing. In it we will perform the final formatting in our data prior to training the model. As well, we will train and save each one of the models.\n",
    "- `src/3_evaluate-model.ipynb`: This notebook handles the evaluation and plots the results for a better understanding of the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_files = glob.glob(r'[0-9]*.ipynb', recursive=None)\n",
    "pipeline_files = np.sort(pipeline_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0_preprocessing.ipynb', '1_data-augmentation.ipynb',\n",
       "       '2_model-training.ipynb', '3_evaluate-model.ipynb'], dtype='<U25')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, node in enumerate(pipeline_files):\n",
    "    print(\"--- Pipeline {}/{}\".format(i+1, len(pipeline_files)))\n",
    "    print(\"Running notebook: \", node)\n",
    "    pm.execute_notebook(node, \"tmp/papermill_tmp.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e9308924db3450c828d689c970f8922fbaf92d5fecac9cb4c2acf46e71fd27c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('amphora-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
